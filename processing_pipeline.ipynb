{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "processing_pipeline.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "HSR_T8nZR15q"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk import pos_tag\n",
        "import itertools\n",
        "import pickle\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('book')"
      ],
      "metadata": {
        "id": "_CxuZ8N6cfMa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sub_hashtags(tweet):\n",
        "\tclean_tweet = re.sub('#[A-Za-z0-9_]+', 'htag', tweet)\n",
        "\treturn clean_tweet"
      ],
      "metadata": {
        "id": "-kyo1AyccEti"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sub_cashtags(tweet):\n",
        "\tclean_tweet = re.sub('\\$[A-Za-z0-9_]+', 'ctag', tweet)\n",
        "\treturn clean_tweet"
      ],
      "metadata": {
        "id": "-d-EAskPcH8N"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sub_user(tweet):\n",
        "\tclean_tweet = re.sub('@[A-Za-z0-9_]+', 'user', tweet)\n",
        "\treturn clean_tweet"
      ],
      "metadata": {
        "id": "rxXXKadOcKZU"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sub_urls(tweet):\n",
        "\tclean_tweet = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', 'url', tweet)\n",
        "\treturn clean_tweet"
      ],
      "metadata": {
        "id": "15WPcTWocMtm"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def delete_punctuation(tweet):\n",
        "\tclean_tweet = re.compile('[%s]' % re.escape(string.punctuation)).sub('', tweet)\n",
        "\treturn clean_tweet"
      ],
      "metadata": {
        "id": "Fzk6lbDccOXQ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sub_numbers(tweet):\n",
        "\tclean_tweet = re.sub('[0-9_]+', 'xyz', tweet)\n",
        "\treturn clean_tweet"
      ],
      "metadata": {
        "id": "QXZRuqxjcQcK"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sub_repeated_chars(tweet):\n",
        "\tclean_tweet = re.sub(r'(.)\\1{2,}', r'\\1', tweet)\n",
        "\treturn clean_tweet"
      ],
      "metadata": {
        "id": "Yqaqcb5QcR2v"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pre_process_pipeline(tweets):\n",
        "\tclean_tweets = []\n",
        "\n",
        "\t# limpiamos todos los tweets\n",
        "\n",
        "\tfor tweet in tweets:\n",
        "\t\tp_tweet = sub_hashtags(tweet)\n",
        "\t\tp_tweet = sub_cashtags(p_tweet)\n",
        "\t\tp_tweet = sub_user(p_tweet)\n",
        "\t\tp_tweet = sub_urls(p_tweet)\n",
        "\t\tp_tweet = delete_punctuation(p_tweet)\n",
        "\t\tp_tweet = sub_numbers(p_tweet)\n",
        "\t\tp_tweet = sub_repeated_chars(p_tweet)\n",
        "\t\tclean_tweets.append(p_tweet)\n",
        "\n",
        "\treturn clean_tweets"
      ],
      "metadata": {
        "id": "i4UbEcyWcUD5"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_tweets(tweets):\n",
        "\ttok_tweets = []\n",
        "\n",
        "\tfor tweet in tweets:\n",
        "\t\ttok_tweets.append(word_tokenize(tweet))\n",
        "\n",
        "\treturn tok_tweets"
      ],
      "metadata": {
        "id": "_9yeSHghcYoa"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lower_case(tweets):\n",
        "\tlow_tweets = []\n",
        "\n",
        "\tfor tweets in tweets:\n",
        "\t\tlow_tweets.append([j.lower() for j in tweets])\n",
        "\n",
        "\treturn low_tweets"
      ],
      "metadata": {
        "id": "09vWM486cahP"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_stopwords(tweets, additional_stp = None):\n",
        "\n",
        "\tstop_words = list(stopwords.words('spanish'))\n",
        "\n",
        "\tstop_words.append('user')\n",
        "\tstop_words.append('htag')\n",
        "\tstop_words.append('ctag')\n",
        "\tstop_words.append('xyz')\n",
        "\tstop_words.append('mail')\n",
        "\tstop_words.append('url')\n",
        "\tstop_words.append('')\n",
        "\tstop_words.append('rt')\n",
        "\tstop_words.append('qt')\n",
        "\n",
        "\tif(additional_stp is not None):\n",
        "\t\tfor item in additional_stp:\n",
        "\t\t\tstop_words.append(item)\n",
        "\n",
        "\tfiltered_tweet = []\n",
        "\n",
        "\tfor tweet in tweets:\n",
        "\t\tfiltered_tweet.append([w for w in tweet if not w in stop_words])\n",
        "\n",
        "\treturn filtered_tweet"
      ],
      "metadata": {
        "id": "HPMUTbUZcc6e"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def stemming(tweets):\n",
        "\n",
        "\tps = PorterStemmer()\n",
        "\n",
        "\tstem_tweets = []\n",
        "\n",
        "\tfor tweet in tweets:\n",
        "\t\tstem_tweets.append([ps.stem(j) for j in tweet])\n",
        "\n",
        "\treturn stem_tweets"
      ],
      "metadata": {
        "id": "pJhjvxRhdBd8"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_wordnet_pos(treebank_tag):\n",
        "\n",
        "\tif treebank_tag.startswith('J'):\n",
        "\t\treturn wn.ADJ\n",
        "\telif treebank_tag.startswith('V'):\n",
        "\t\treturn wn.VERB\n",
        "\telif treebank_tag.startswith('N'):\n",
        "\t\treturn wn.NOUN\n",
        "\telif treebank_tag.startswith('R'):\n",
        "\t\treturn wn.ADV\n",
        "\telse:\n",
        "\t\treturn wn.NOUN"
      ],
      "metadata": {
        "id": "05ELeWwBdCys"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def eng_lemmatizer(tweets):\n",
        "\n",
        "\twnl = WordNetLemmatizer()\n",
        "\n",
        "\tlem_tweets = []\n",
        "\n",
        "\tfor tweet in tweets:\n",
        "\t\ttags = pos_tag(tweet)\n",
        "\t\tlem_tweets.append([wnl.lemmatize(j[0],get_wordnet_pos(j[1])) for j in tags])\n",
        "\n",
        "\treturn lem_tweets"
      ],
      "metadata": {
        "id": "giDNtqj0dCo1"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_tweets(file_name):\n",
        "\n",
        "\ttweets = []\n",
        "\n",
        "\twith open(file_name, 'r') as csv_file:\n",
        "\t\tcsv_reader = csv.DictReader(csv_file, delimiter = ';')\n",
        "\t\tfor row in csv_reader:\n",
        "\t\t\ttweets.append(row['text'])\n",
        "\n",
        "\t# preprocesamiento de tweets\n",
        "\tprep_tweets = pre_process_pipeline(tweets)\n",
        "\n",
        "\t# procesamiento de tweets - tokenize\n",
        "\ttok_tweets = tokenize_tweets(prep_tweets)\n",
        "\n",
        "\t# procesamiento de tweets - letra minuscula\n",
        "\tlow_tweets = lower_case(tok_tweets)\n",
        "\n",
        "\t# procesamiento de tweets - filtado de palabras vacias\n",
        "\tstop_tweets = filter_stopwords(low_tweets)\n",
        "\n",
        "\t# procesamiento de tweets - stemming\n",
        "\tstm_tweets = stemming(stop_tweets)\n",
        "\n",
        "\t# se guardan los tweets procesados en un archivo pickle para su uso posterior\n",
        "\twith open('canelo.pickle', 'wb') as f:\n",
        "\t\tpickle.dump(stm_tweets, f, pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "\treturn stm_tweets"
      ],
      "metadata": {
        "id": "GswIXYzdd_Lz"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stm_tweets = process_tweets('ukraine.csv')"
      ],
      "metadata": {
        "id": "5Yji4ga-URiN"
      },
      "execution_count": 25,
      "outputs": []
    }
  ]
}
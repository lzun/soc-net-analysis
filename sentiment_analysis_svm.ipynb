{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sentiment-analysis-svm.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk import pos_tag\n",
        "import itertools\n",
        "import pickle\n",
        "import gensim\n",
        "from gensim import corpora, models\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict, cross_validate, RepeatedStratifiedKFold\n",
        "from sklearn.metrics import recall_score, f1_score, accuracy_score, precision_score\n",
        "from sklearn.utils import class_weight\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.decomposition import PCA"
      ],
      "metadata": {
        "id": "-oiH4q3TaFRo"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('book')"
      ],
      "metadata": {
        "id": "_CxuZ8N6cfMa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sub_hashtags(tweet):\n",
        "\tclean_tweet = re.sub('#[A-Za-z0-9_]+', 'htag', tweet)\n",
        "\treturn clean_tweet"
      ],
      "metadata": {
        "id": "-kyo1AyccEti"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sub_cashtags(tweet):\n",
        "\tclean_tweet = re.sub('\\$[A-Za-z0-9_]+', 'ctag', tweet)\n",
        "\treturn clean_tweet"
      ],
      "metadata": {
        "id": "-d-EAskPcH8N"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sub_user(tweet):\n",
        "\tclean_tweet = re.sub('@[A-Za-z0-9_]+', 'user', tweet)\n",
        "\treturn clean_tweet"
      ],
      "metadata": {
        "id": "rxXXKadOcKZU"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sub_urls(tweet):\n",
        "\tclean_tweet = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', 'url', tweet)\n",
        "\treturn clean_tweet"
      ],
      "metadata": {
        "id": "15WPcTWocMtm"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def delete_punctuation(tweet):\n",
        "\tclean_tweet = re.compile('[%s]' % re.escape(string.punctuation)).sub('', tweet)\n",
        "\treturn clean_tweet"
      ],
      "metadata": {
        "id": "Fzk6lbDccOXQ"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sub_numbers(tweet):\n",
        "\tclean_tweet = re.sub('[0-9_]+', 'xyz', tweet)\n",
        "\treturn clean_tweet"
      ],
      "metadata": {
        "id": "QXZRuqxjcQcK"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sub_repeated_chars(tweet):\n",
        "\tclean_tweet = re.sub(r'(.)\\1{2,}', r'\\1', tweet)\n",
        "\treturn clean_tweet"
      ],
      "metadata": {
        "id": "Yqaqcb5QcR2v"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pre_process_pipeline(tweets):\n",
        "\tclean_tweets = []\n",
        "\n",
        "\t# limpiamos todos los tweets\n",
        "\n",
        "\tfor tweet in tweets:\n",
        "\t\tp_tweet = sub_hashtags(tweet)\n",
        "\t\tp_tweet = sub_cashtags(p_tweet)\n",
        "\t\tp_tweet = sub_user(p_tweet)\n",
        "\t\tp_tweet = sub_urls(p_tweet)\n",
        "\t\tp_tweet = delete_punctuation(p_tweet)\n",
        "\t\tp_tweet = sub_numbers(p_tweet)\n",
        "\t\tp_tweet = sub_repeated_chars(p_tweet)\n",
        "\t\tclean_tweets.append(p_tweet)\n",
        "\n",
        "\treturn clean_tweets"
      ],
      "metadata": {
        "id": "i4UbEcyWcUD5"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_tweets(tweets):\n",
        "\ttok_tweets = []\n",
        "\n",
        "\tfor tweet in tweets:\n",
        "\t\ttok_tweets.append(word_tokenize(tweet))\n",
        "\n",
        "\treturn tok_tweets"
      ],
      "metadata": {
        "id": "_9yeSHghcYoa"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lower_case(tweets):\n",
        "\tlow_tweets = []\n",
        "\n",
        "\tfor tweets in tweets:\n",
        "\t\tlow_tweets.append([j.lower() for j in tweets])\n",
        "\n",
        "\treturn low_tweets"
      ],
      "metadata": {
        "id": "09vWM486cahP"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_stopwords(tweets, additional_stp = None):\n",
        "\n",
        "\tstop_words = list(stopwords.words('spanish'))\n",
        "\n",
        "\tstop_words.append('user')\n",
        "\tstop_words.append('htag')\n",
        "\tstop_words.append('ctag')\n",
        "\tstop_words.append('xyz')\n",
        "\tstop_words.append('mail')\n",
        "\tstop_words.append('url')\n",
        "\tstop_words.append('')\n",
        "\tstop_words.append('rt')\n",
        "\tstop_words.append('qt')\n",
        "\n",
        "\tif(additional_stp is not None):\n",
        "\t\tfor item in additional_stp:\n",
        "\t\t\tstop_words.append(item)\n",
        "\n",
        "\tfiltered_tweet = []\n",
        "\n",
        "\tfor tweet in tweets:\n",
        "\t\tfiltered_tweet.append([w for w in tweet if not w in stop_words])\n",
        "\n",
        "\treturn filtered_tweet"
      ],
      "metadata": {
        "id": "HPMUTbUZcc6e"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def stemming(tweets):\n",
        "\n",
        "\tps = PorterStemmer()\n",
        "\n",
        "\tstem_tweets = []\n",
        "\n",
        "\tfor tweet in tweets:\n",
        "\t\tstem_tweets.append([ps.stem(j) for j in tweet])\n",
        "\n",
        "\treturn stem_tweets"
      ],
      "metadata": {
        "id": "pJhjvxRhdBd8"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_wordnet_pos(treebank_tag):\n",
        "\n",
        "\tif treebank_tag.startswith('J'):\n",
        "\t\treturn wn.ADJ\n",
        "\telif treebank_tag.startswith('V'):\n",
        "\t\treturn wn.VERB\n",
        "\telif treebank_tag.startswith('N'):\n",
        "\t\treturn wn.NOUN\n",
        "\telif treebank_tag.startswith('R'):\n",
        "\t\treturn wn.ADV\n",
        "\telse:\n",
        "\t\treturn wn.NOUN"
      ],
      "metadata": {
        "id": "05ELeWwBdCys"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def eng_lemmatizer(tweets):\n",
        "\n",
        "\twnl = WordNetLemmatizer()\n",
        "\n",
        "\tlem_tweets = []\n",
        "\n",
        "\tfor tweet in tweets:\n",
        "\t\ttags = pos_tag(tweet)\n",
        "\t\tlem_tweets.append([wnl.lemmatize(j[0],get_wordnet_pos(j[1])) for j in tags])\n",
        "\n",
        "\treturn lem_tweets"
      ],
      "metadata": {
        "id": "giDNtqj0dCo1"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_tweets(file_name):\n",
        "\n",
        "\ttweets = []\n",
        "\tsentiment = []\n",
        "\n",
        "\twith open(file_name, 'r') as csv_file:\n",
        "\t\tcsv_reader = csv.DictReader(csv_file, delimiter = ',')\n",
        "\t\tfor row in csv_reader:\n",
        "\t\t\ttweets.append(row['text'])\n",
        "\t\t\tsentiment.append(int(row['text_sentiment']))\n",
        "\n",
        "\t# preprocesamiento de tweets\n",
        "\tprep_tweets = pre_process_pipeline(tweets)\n",
        "\n",
        "\t# procesamiento de tweets - tokenize\n",
        "\ttok_tweets = tokenize_tweets(prep_tweets)\n",
        "\n",
        "\t# procesamiento de tweets - letra minuscula\n",
        "\tlow_tweets = lower_case(tok_tweets)\n",
        "\n",
        "\t# procesamiento de tweets - filtado de palabras vacias\n",
        "\tstop_tweets = filter_stopwords(low_tweets)\n",
        "\n",
        "\t# procesamiento de tweets - stemming\n",
        "\tstm_tweets = stemming(stop_tweets)\n",
        "\n",
        "\t# se guardan los tweets procesados en un archivo pickle para su uso posterior\n",
        "\twith open('canelo.pickle', 'wb') as f:\n",
        "\t\tpickle.dump(stm_tweets, f, pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "\treturn stm_tweets, sentiment"
      ],
      "metadata": {
        "id": "GswIXYzdd_Lz"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot2_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n",
        "\t\"\"\"\n",
        "\tThis function prints and plots the confusion matrix.\n",
        "\tNormalization can be applied by setting `normalize=True`.\n",
        "\t\"\"\"\n",
        "\tif normalize:\n",
        "\t\tcm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\t\tprint(\"Normalized confusion matrix\")\n",
        "\telse:\n",
        "\t\tprint('Confusion matrix, without normalization')\n",
        "\n",
        "\tprint(cm)\n",
        "\n",
        "\tplt.imshow(cm, interpolation='nearest', cmap=cmap, aspect='auto')\n",
        "\tplt.title(title)\n",
        "\tplt.colorbar()\n",
        "\ttick_marks = np.arange(len(classes))\n",
        "\tplt.xticks(tick_marks, classes, rotation=45)\n",
        "\tplt.yticks(tick_marks, classes)\n",
        "\n",
        "\tfmt = '.1f' if normalize else 'd'\n",
        "\tthresh = cm.max() / 2.\n",
        "\tfor i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "\t\tplt.text(j, i, format(cm[i, j], fmt),horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\t\n",
        "\t#plt.tight_layout()\n",
        "\tplt.ylabel('Clase correcta')\n",
        "\tplt.xlabel('Clase predicha')"
      ],
      "metadata": {
        "id": "mUS7SwFRasvC"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bow_to_vector(tweet, dict_len):\n",
        "\t# hacemos un vector de ceros del tamaño del vocabulario\n",
        "\tvector = [0] * dict_len\n",
        "\n",
        "\t# agregamos los valores del bow del tweet\n",
        "\tfor item in tweet:\n",
        "\t\tvector[item[0]] = item[1]\n",
        "\n",
        "\treturn vector"
      ],
      "metadata": {
        "id": "MFSBGskOakDv"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "AHeW64DKZ7Uy"
      },
      "outputs": [],
      "source": [
        "def make_bow(tweets):\n",
        "  # Hacemos la bolsa de palabras\n",
        "\tdictionary = gensim.corpora.Dictionary(tweets)\n",
        "\tdictionary.filter_extremes(no_below=15, no_above=1, keep_n=100000)\n",
        "\tbow_corpus = [dictionary.doc2bow(doc) for doc in tweets]\n",
        "\n",
        "\treturn bow_corpus, len(dictionary)"
      ]
    }
  ]
}